{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset,DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure data reproducibility\n",
    "def random_seed(seed):\n",
    "    random.seed(seed)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] =str(seed)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "\n",
    "#Customize functions\n",
    "#Define functions\n",
    "#Standardization\n",
    "def ss(features, labels):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_s = scaler.fit_transform(features)\n",
    "    X_s = pd.DataFrame(X_s)\n",
    "    data = pd.concat([X_s, labels], axis=1)\n",
    "    return data\n",
    "\n",
    "def model_score(model, x, y, trainsize, testsize):\n",
    "    cv = ShuffleSplit(n_splits=10, train_size=trainsize, test_size=testsize, random_state=0)\n",
    "    rmse = cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv=cv)\n",
    "    rmse_score = np.sqrt(-rmse)\n",
    "    rmse_mean = rmse_score.mean()\n",
    "\n",
    "    mae = cross_val_score(model, x, y, scoring=\"neg_mean_absolute_error\", cv=cv)\n",
    "    mae_score = -mae\n",
    "    mae_mean = mae_score.mean()\n",
    "\n",
    "    r2 = cross_val_score(model, x, y, scoring='r2', cv=cv)\n",
    "    r2_mean = r2.mean()\n",
    "\n",
    "    scores = [rmse_score, rmse_mean, mae_score, mae_mean, r2, r2_mean]\n",
    "\n",
    "    rmse = pd.DataFrame(scores[0], columns=['rmse'], index = [np.arange(len(scores[0]))])\n",
    "    mae = pd.DataFrame(scores[2], columns=['mae'], index = [np.arange(len(scores[2]))])\n",
    "    R2 = pd.DataFrame(scores[4], columns=['R2'], index = [np.arange(len(scores[4]))])\n",
    "    scores_df = pd.concat([rmse,mae,R2], axis=1)\n",
    "    return scores_df\n",
    "\n",
    "\n",
    "def ToCsv(model, Xtest, ytest, filename):\n",
    "    ytest = pd.DataFrame(ytest.values, index=[np.arange(len(ytest))], columns=['yreal1', 'yreal2', 'yreal3'])\n",
    "    ypredict = model.predict(Xtest)\n",
    "    ypredict = pd.DataFrame(ypredict, index=[np.arange(len(ytest))], columns=['ypredict1', 'ypredict2', 'ypredict3'])\n",
    "    output = pd.concat([ytest, ypredict], axis=1)\n",
    "    output.to_csv(filename)\n",
    "\n",
    "def DataProcess(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data_df = pd.DataFrame(data)\n",
    "    X_df = data_df.iloc[:,1:6]\n",
    "    y_df = data_df.iloc[:,6:9]\n",
    "\n",
    "    data_s = ss(X_df,y_df)\n",
    "    X = data_s.iloc[:,0:5]\n",
    "    y = data_s.iloc[:,5:]\n",
    "    return X, y\n",
    "\n",
    "def DataSplit(X,y, testsize, seed):\n",
    "    random_seed(seed)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=testsize)\n",
    "    return Xtrain, Xtest, ytrain, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TL-DNN model\n",
    "\n",
    "#dataframe to tensor\n",
    "def Df2Tensor(df):\n",
    "    array = np.array(df)\n",
    "    tensor = torch.tensor(array, dtype=torch.float32)\n",
    "    return tensor\n",
    "\n",
    "def ToDataset(*args):\n",
    "    return TensorDataset(*args)\n",
    "\n",
    "def ToDataLoader(dataset, batchsize):\n",
    "    return DataLoader(dataset, batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "#Define the network structure\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, \n",
    "            input_dim, output_dim, \n",
    "            hidden_layer1, hidden_layer2, hidden_layer3, hidden_layer4, \n",
    "            dropout1, dropout2, dropout3, dropout4):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,hidden_layer1)\n",
    "        self.layer2 = nn.Linear(hidden_layer1,hidden_layer2)\n",
    "        self.layer3 = nn.Linear(hidden_layer2,hidden_layer3)\n",
    "        self.layer4 = nn.Linear(hidden_layer3,hidden_layer4)\n",
    "        self.layer5 = nn.Linear(hidden_layer4,output_dim)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.dropout3 = nn.Dropout(dropout3)\n",
    "        self.dropout4 = nn.Dropout(dropout4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    "\n",
    "#Define model evaluation parameters\n",
    "class Metrics():\n",
    "    def __init__(self, net, dataloader):\n",
    "        dataset = dataloader.dataset\n",
    "        self.features = dataset[:][0]\n",
    "        self.labels = dataset[:][1]\n",
    "        self.y_hat = torch.clamp(net(self.features), 1, float('inf'))\n",
    "    def rmse(self):\n",
    "        return torch.sqrt(F.mse_loss(self.y_hat, self.labels))\n",
    "    def mae(self):\n",
    "        return F.l1_loss(self.y_hat, self.labels)\n",
    "    def smooth_mae(self):\n",
    "        return F.smooth_l1_loss(self.y_hat, self.labels)\n",
    "    def r2(self):\n",
    "        SS_res = torch.sum(torch.square(self.labels-self.y_hat))\n",
    "        SS_tot = torch.sum(torch.square(self.labels - torch.mean(self.labels)))\n",
    "        r2 = 1 - SS_res / SS_tot\n",
    "        return r2 \n",
    "\n",
    "def init_weights(m):\n",
    "  if type(m) == nn.Linear:\n",
    "    nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "def DataConcat(Xtrain, Xtest, ytrain, ytest):\n",
    "    train_df = [Xtrain, ytrain]\n",
    "    test_df = [Xtest, ytest]\n",
    "    train_data = pd.concat(train_df,axis=1)\n",
    "    test_data = pd.concat(test_df,axis=1)\n",
    "    return train_data, test_data\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train(net, dataloader, loss, num_epochs, lr, wd):\n",
    "    net.train()\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr, weight_decay = wd)\n",
    "    scheduler = StepLR(optimizer, step_size=num_epochs/3, gamma=0.3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            l = loss(net(X), y) \n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "#model evaluation\n",
    "def NetEval(net, dataloader, num_epochs, loss, lr, wd):\n",
    "    eval_list = []\n",
    "    rmse, mae, r2 = [], [], []\n",
    "    for epochs in range(num_epochs):\n",
    "        net.train()\n",
    "        train(net, dataloader, loss, num_epochs, lr, wd)\n",
    "\n",
    "        net.eval()\n",
    "        test_metrics = Metrics(net, dataloader)\n",
    "\n",
    "\n",
    "        rmse.append(test_metrics.rmse().detach().item())\n",
    "        mae.append(test_metrics.mae().detach().item())\n",
    "        r2.append(test_metrics.r2().detach().item())\n",
    "    return r2, mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data set preprocessing\n",
    "Path = \"FEA_data.csv\"\n",
    "\n",
    "seed = 0\n",
    "trainsize, testsize = 0.1,0.9\n",
    "\n",
    "X, y = DataProcess(Path)\n",
    "Xtrain, Xtest, ytrain, ytest = DataSplit(X,y, testsize, seed)\n",
    "\n",
    "#Data preprocessing\n",
    "train_dataset = ToDataset(Df2Tensor(Xtrain), Df2Tensor(ytrain))\n",
    "test_dataset = ToDataset(Df2Tensor(Xtest), Df2Tensor(ytest))\n",
    "\n",
    "batchsize = 54\n",
    "train_dataloader = ToDataLoader(train_dataset, batchsize)\n",
    "test_dataloader = ToDataLoader(test_dataset, batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer-Learning\n",
    "AM_Path = 'AM_data.csv'\n",
    "seed = 0\n",
    "trainsize1, testsize1 = 0.99,0.01\n",
    "X, y = DataProcess(AM_Path)\n",
    "Xtrain1, Xtest1, ytrain1, ytest1 = DataSplit(X,y, testsize1, seed)\n",
    "\n",
    "train_dataset1 = ToDataset(Df2Tensor(Xtrain1), Df2Tensor(ytrain1))\n",
    "test_dataset1 = ToDataset(Df2Tensor(Xtest1), Df2Tensor(ytest1))\n",
    "\n",
    "batchsize = 54\n",
    "train_dataloader1 = ToDataLoader(train_dataset1, batchsize)\n",
    "test_dataloader1= ToDataLoader(test_dataset1, batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer1): Linear(in_features=5, out_features=120, bias=True)\n",
       "  (layer2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (layer3): Linear(in_features=60, out_features=30, bias=True)\n",
       "  (layer4): Linear(in_features=30, out_features=15, bias=True)\n",
       "  (layer5): Linear(in_features=15, out_features=3, bias=True)\n",
       "  (dropout1): Dropout(p=0, inplace=False)\n",
       "  (dropout2): Dropout(p=0.01, inplace=False)\n",
       "  (dropout3): Dropout(p=0.01, inplace=False)\n",
       "  (dropout4): Dropout(p=0.01, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define hyperparameters and the network\n",
    "input_dim, output_dim, hidden_layer1, hidden_layer2, hidden_layer3, hidden_layer4 = 5, 3, 120,60,30,15\n",
    "num_epochs, lr, wd, batch_size = 1000, 0.003, 0, 54\n",
    "dropout1, dropout2, dropout3, dropout4 = 0,0.01,0.01,0.01\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "net = Net(input_dim, output_dim, \n",
    "            hidden_layer1, hidden_layer2, hidden_layer3, hidden_layer4,\n",
    "            dropout1, dropout2, dropout3, dropout4)\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-training\n",
    "train(net, train_dataloader1, loss, num_epochs, lr, wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the network\n",
    "train(net, train_dataloader, loss, num_epochs, lr, wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTNN R2交叉验证平均值:0.9834182858467102\n",
      "DTNN mae交叉验证平均值:2.777275896072388\n",
      "DTNN rmse交叉验证平均值:4.077270460128784\n"
     ]
    }
   ],
   "source": [
    "#Model evaluation\n",
    "eval_epochs = 10\n",
    "wd = 0\n",
    "r2, mae, rmse = NetEval(net, test_dataloader, eval_epochs, loss, lr, wd)\n",
    "\n",
    "DTNN_r2_mean = np.mean(r2)\n",
    "DTNN_mae_mean = np.mean(mae)\n",
    "DTNN_rmse_mean = np.mean(rmse)\n",
    "\n",
    "print('The average R² of TL-DNN:{}'.format(DTNN_r2_mean))\n",
    "print('the average MAE:{}'.format(DTNN_mae_mean))\n",
    "print('the average RMSE:{}'.format(DTNN_rmse_mean))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
